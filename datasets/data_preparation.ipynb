{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e2196b",
   "metadata": {},
   "source": [
    "# This is the part where the data handling pipeline is created.\n",
    "- here data will be turned into json format for spacy training.\n",
    "- Pandas will be used for cleaning and preparation of the data.\n",
    "- Then they will be put through preatrained spacy model and then NER will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5261dd",
   "metadata": {},
   "source": [
    "## Step 1. Importing Libraries/Frameworks\n",
    "- Formatting of the datasets into one format (JSON).\n",
    "- save it in a docbin format\n",
    "- Then feed it to a NER model (spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c896c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import chardet\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b052b",
   "metadata": {},
   "source": [
    "### Open a new directory to save the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4307e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"NER_ready_data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3c2c4",
   "metadata": {},
   "source": [
    "## Step 2. Handling Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e452f",
   "metadata": {},
   "source": [
    "### 2.1 Preparing CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05d02a",
   "metadata": {},
   "source": [
    "#### Loading datasets\n",
    "\n",
    "- Since all of them has unique structure and naming I will open them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26fde5",
   "metadata": {},
   "source": [
    "#### Dataset 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a5212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"ahmedheakl_resume_atlas.csv\")\n",
    "df = pd.read_csv(data_path, index_col='Unnamed: 0')\n",
    "df['Category'].unique()\n",
    "it_jobs_list = [\"Blockchain\", \"Data Science\", \"Database\", \"DevOps\", \"DotNet Developer\", \"ETL Developer\", \"Information Technology\", \"Java Developer\", \n",
    "                \"Network Security Engineer\", \"Python Developer\", \"React Developer\", \"SAP Developer\", \"SQL Developer\", \"Web Designing\"]\n",
    "df = df[df['Category'].isin(it_jobs_list)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "## dataset does not contain empty values so there is no need to do anything more for spaCy training.\n",
    "file_path = out_dir / \"dataset1.csv\"\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d06088",
   "metadata": {},
   "source": [
    "#### Dataset 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ceadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"InferencePrince555_Resume_data.csv\")\n",
    "df = pd.read_csv(data_path, index_col='Unnamed: 0')\n",
    "df = df.drop(columns=['input','instruction']).reset_index(drop=True)\n",
    "## this one does not have an empty vals so this one directly goes to \n",
    "file_path = out_dir / \"dataset2.csv\"\n",
    "df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142bd78",
   "metadata": {},
   "source": [
    "#### Dataset 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac4653f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Bytes: b',Category,Resume\\n0,Data Science,\"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, Na\\xc3\\x83\\xc2\\xafve Ba'\n",
      "has utf-8 BOM: False\n",
      "Contains NUL bytes: False\n"
     ]
    }
   ],
   "source": [
    "p = os.path.join(os.getcwd(), \"Sachinkelenjaguri_resume_Dataset.csv\")\n",
    "with open(p, \"rb\") as fh:\n",
    "    head = fh.read(200)\n",
    "print(\"First Bytes:\", head)\n",
    "print(\"has utf-8 BOM:\", head.startswith(b\"\\xef\\xbb\\xbf\"))\n",
    "\n",
    "has_nul = b\"\\x00\" in head\n",
    "print(\"Contains NUL bytes:\", has_nul)\n",
    "\n",
    "df = pd.read_csv(\"Sachinkelenjaguri_resume_Dataset.csv\", encoding=\"utf-8\", low_memory=False, index_col='Unnamed: 0')\n",
    "df['Category'] = df['Category'].astype(str).apply(ftfy.fix_text)\n",
    "df[\"Resume\"] = df[\"Resume\"].astype(str).apply(ftfy.fix_text)\n",
    "\n",
    "data_path = out_dir / \"dataset3.csv\"\n",
    "df.to_csv(data_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e2bbd",
   "metadata": {},
   "source": [
    "### 2.2 handling data from the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5297e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'PosixPath'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df = pd.read_csv(dataset_path, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m df = df.drop(columns=\u001b[33m'\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m'\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset4.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m df.to_csv(data_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'str' and 'PosixPath'"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"wahib04/multilabel-resume-dataset/versions/1/data.csv\")\n",
    "df = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "df = df.drop(columns='Label').reset_index(drop=True)\n",
    "\n",
    "data_path = out_dir / \"dataset4.csv\"\n",
    "df.to_csv(data_path, index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job-Recommender (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
