{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e2196b",
   "metadata": {},
   "source": [
    "# This is the part where the data handling pipeline is created.\n",
    "- here data will be turned into json format for spacy training.\n",
    "- Pandas will be used for cleaning and preparation of the data.\n",
    "- Then they will be put through preatrained spacy model and then NER will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5261dd",
   "metadata": {},
   "source": [
    "## Step 1. Importing Libraries/Frameworks\n",
    "- Formatting of the datasets into one format (JSON).\n",
    "- save it in a docbin format\n",
    "- Then feed it to a NER model (spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c896c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import chardet\n",
    "import ftfy\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b052b",
   "metadata": {},
   "source": [
    "### Open a new directory to save the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4307e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"NER_ready_data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3c2c4",
   "metadata": {},
   "source": [
    "## Step 2. Handling Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e452f",
   "metadata": {},
   "source": [
    "### 2.1 Preparing CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05d02a",
   "metadata": {},
   "source": [
    "#### Loading datasets\n",
    "\n",
    "- Since all of them has unique structure and naming I will open them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26fde5",
   "metadata": {},
   "source": [
    "#### Dataset 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a5212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"ahmedheakl_resume_atlas.csv\")\n",
    "df = pd.read_csv(data_path, index_col='Unnamed: 0')\n",
    "df['Category'].unique()\n",
    "it_jobs_list = [\"Blockchain\", \"Data Science\", \"Database\", \"DevOps\", \"DotNet Developer\", \"ETL Developer\", \"Information Technology\", \"Java Developer\", \n",
    "                \"Network Security Engineer\", \"Python Developer\", \"React Developer\", \"SAP Developer\", \"SQL Developer\", \"Web Designing\"]\n",
    "df = df[df['Category'].isin(it_jobs_list)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "## dataset does not contain empty values so there is no need to do anything more for spaCy training.\n",
    "file_path = out_dir / \"dataset1.csv\"\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d06088",
   "metadata": {},
   "source": [
    "#### Dataset 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ceadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"InferencePrince555_Resume_data.csv\")\n",
    "df = pd.read_csv(data_path, index_col='Unnamed: 0')\n",
    "df = df.drop(columns=['input','instruction']).reset_index(drop=True)\n",
    "## this one does not have an empty vals so this one directly goes to \n",
    "file_path = out_dir / \"dataset2.csv\"\n",
    "df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142bd78",
   "metadata": {},
   "source": [
    "#### Dataset 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4653f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Bytes: b',Category,Resume\\n0,Data Science,\"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, Na\\xc3\\x83\\xc2\\xafve Ba'\n",
      "has utf-8 BOM: False\n",
      "Contains NUL bytes: False\n"
     ]
    }
   ],
   "source": [
    "p = os.path.join(os.getcwd(), \"Sachinkelenjaguri_resume_Dataset.csv\")\n",
    "with open(p, \"rb\") as fh:\n",
    "    head = fh.read(200)\n",
    "print(\"First Bytes:\", head)\n",
    "print(\"has utf-8 BOM:\", head.startswith(b\"\\xef\\xbb\\xbf\"))\n",
    "\n",
    "has_nul = b\"\\x00\" in head\n",
    "print(\"Contains NUL bytes:\", has_nul)\n",
    "\n",
    "df = pd.read_csv(\"Sachinkelenjaguri_resume_Dataset.csv\", encoding=\"utf-8\", low_memory=False, index_col='Unnamed: 0')\n",
    "df['Category'] = df['Category'].astype(str).apply(ftfy.fix_text)\n",
    "df[\"Resume\"] = df[\"Resume\"].astype(str).apply(ftfy.fix_text)\n",
    "\n",
    "data_path = out_dir / \"dataset3.csv\"\n",
    "df.to_csv(data_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e2bbd",
   "metadata": {},
   "source": [
    "### 2.2 handling data from the directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4615a18",
   "metadata": {},
   "source": [
    "#### Dataset 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a5297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"wahib04/multilabel-resume-dataset/versions/1/data.csv\")\n",
    "df = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "df = df.drop(columns='Label').reset_index(drop=True)\n",
    "\n",
    "data_path = out_dir / \"dataset4.csv\"\n",
    "df.to_csv(data_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77d3d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['05_person_skills.csv', '03_education.csv', '06_skills.csv', '04_experience.csv', '02_abilities.csv', '01_people.csv']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"suriyaganesh/resume-dataset-structured/versions/2\")\n",
    "print(os.listdir(dataset_path))\n",
    "\n",
    "df1_path = os.path.join(dataset_path, \"01_people.csv\")\n",
    "df1 = pd.read_csv(df1_path)\n",
    "\n",
    "df2_path = os.path.join(dataset_path, \"02_abilities.csv\")\n",
    "df2 = pd.read_csv(df2_path)\n",
    "\n",
    "df3_path = os.path.join(dataset_path, \"03_education.csv\")\n",
    "df3 = pd.read_csv(df3_path)\n",
    "\n",
    "df4_path = os.path.join(dataset_path, \"04_experience.csv\")\n",
    "df4 = pd.read_csv(df4_path)\n",
    "\n",
    "df5_path = os.path.join(dataset_path, \"05_person_skills.csv\")\n",
    "df5 = pd.read_csv(df5_path)\n",
    "\n",
    "df6_path = os.path.join(dataset_path, \"06_skills.csv\")\n",
    "df6 = pd.read_csv(df6_path)\n",
    "\n",
    "merged_1 = df1.merge(df2, on=\"person_id\", how=\"inner\")\n",
    "merged_1 = merged_1.drop(columns=[\"email\", \"phone\", \"linkedin\"]).reset_index(drop=True)\n",
    "\n",
    "rest_dfs = [df3, df4, df5]\n",
    "merged_2 = reduce(lambda left, right: pd.merge(left, right, on=\"person_id\", how=\"inner\"), rest_dfs)\n",
    "merged_2 = merged_2.drop(columns=\"location_x\").reset_index(drop=True)\n",
    "\n",
    "merged_3 = merged_2.merge(df6, on=\"skill\", how=\"inner\")\n",
    "\n",
    "merged_3[\"program\"] = merged_3[\"program\"].fillna(\"Not attended to University\")\n",
    "\n",
    "merged_reduced = merged_3.groupby([\"person_id\"]).agg({\n",
    "    \"program\": lambda x: ', '.join(sorted(set(x.dropna().astype(str)))), \n",
    "    \"title\": lambda x: ', '.join(sorted(set(x.dropna().astype(str)))), \n",
    "    \"firm\": lambda x: ', '.join(sorted(set(x.dropna().astype(str)))), \n",
    "    \"skill\": lambda x: ', '.join(sorted(set(x.dropna().astype(str))))}\n",
    "    ).reset_index()\n",
    "\n",
    "merged_reduced[\"resume\"] = np.where(\n",
    "    merged_reduced[\"program\"] != \"Not attended to University\",\n",
    "    \"Candidate \" + merged_reduced[\"person_id\"].astype(str)\n",
    "    + \", has completed \" + merged_reduced[\"program\"]\n",
    "    + \", and worked in the following positions: \" + merged_reduced[\"title\"]\n",
    "    + \", at the following companies: \" + merged_reduced[\"firm\"]\n",
    "    + \", has skills: \" + merged_reduced[\"skill\"],\n",
    "    \n",
    "    \"Candidate \" + merged_reduced[\"person_id\"].astype(str)\n",
    "    + \", has not attended university, and worked in the following positions: \"\n",
    "    + merged_reduced[\"title\"]\n",
    "    + \", at the following companies: \" + merged_reduced[\"firm\"]\n",
    "    + \", has skills: \" + merged_reduced[\"skill\"]\n",
    ")\n",
    "\n",
    "final_set = merged_reduced[[\"person_id\", \"resume\"]]\n",
    "\n",
    "final_path = out_dir / \"dataset5.csv\"\n",
    "final_set.to_csv(final_path, index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job-Recommender (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
